---
title: "MZES SSDL Input Talk: Collection, Management, and Analysis of Twitter Data: Using the Twitter API for Academic Research and BERT (Andreas KÃ¼pfer, TU Darmstadt & MZES)"
date: 2022-05-04
output: html_notebook
---

```{r}
## Save package names as a vector of strings
pkgs <- c("dplyr", "academictwitteR", "quanteda", "purrr")

## Install uninstalled packages
lapply(pkgs[!(pkgs %in% installed.packages())], install.packages)

## Load all packages to library and adjust options
lapply(pkgs, library, character.only = TRUE)

# setting the token used for authentication
academictwitteR::set_bearer()
```

```{r}
german_mps <- read.csv("data/MP_de_twitter_uid.csv",
                       colClasses=c("user_id"="character"))
head(german_mps)
```


```{r}
# function to retrieve tweets in a specific time period of a single user
# (list of user IDs would be possible but one should keep
# the max. query string of 1024 characters in mind)

get_tweets_from_user <- function(user_id) {
  # Another option is to add "query" parameter
  academictwitteR::get_all_tweets(
    users = user_id,
    start_tweets = "2021-01-01T00:00:00Z",
    end_tweets = "2021-09-30T00:00:00Z",
    data_path = "data/raw/",
    n = 100)
}

# call function for each MP in the list
purrr::walk(german_mps[["user_id"]], get_tweets_from_user)

# in case of an interruption in between the data collection
# of a user resume the collection process:
# resume_collection(data_path = "data/raw/")

# in case you want to update a set of crawled tweets
# update the collection until a specified end date:
# update_collection(data_path = "data/raw/", end_tweets = "2022-04-30T00:00:00Z")
```

```{r}
# concatenate all retrieved tweets into one dataframe and select which columns
# should be kept
# Another option: set parameter "user" to TRUE to retrieve user information
tweets_df <- academictwitteR::bind_tweets(data_path = "data/raw/",
                                          output_format = "tidy") %>%
  dplyr::select(
    tweet_id,
    text,
    author_id,
    user_username,
    created_at,
    sourcetweet_type,
    sourcetweet_text,
    lang
  )
```

```{r}
# Store the tweets as .csv
write.csv(tweets_df, "data/raw/tweets_german_mp.csv", row.names = FALSE)
```

```{r}
# creates a corpus and sets tweet_ids as document names to re-identify
tweet_corpus <- quanteda::corpus(tweets_df[["text"]],
                                 docnames = tweets_df[["tweet_id"]])

# "2020 wurden in Berlin ca. 18.800 Miet-
# in Eigentumswohnungen umgewandelt. #Umwandlungsverbot"
dfm <-
  quanteda::dfm(tweet_corpus %>%
                  quanteda::tokens(
                    remove_punct = TRUE,
                    remove_numbers = TRUE)) %>%
  quanteda::dfm_tolower() %>% # removes capitialization
  quanteda::dfm_remove(
    stopwords("german")) %>% # removes German stopwords
  quanteda::dfm_wordstem(
    language = "german") # transforms words to their German wordstems
# "wurd berlin ca miet- eigentumswohn umgewandelt #umwandlungsverbot"

head(dfm)
```

